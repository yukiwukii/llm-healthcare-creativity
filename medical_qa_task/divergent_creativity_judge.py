import pandas as pd
import numpy as np
from tqdm import tqdm
from dotenv import load_dotenv
from openai import OpenAI
import os
import re

# File paths
FILENAME = 'hallucination/checkpoint.csv'
SAVEFILE = 'creativity/creativity_results.csv'
CHECKPOINT = 'creativity/checkpoint_creativity.csv'

# Load the dataset
df_full = pd.read_csv(FILENAME)

# Helper functions
def initialize_judge(): 
    load_dotenv()
    client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
    return client

def write_prompt(idx):
    qn = df.loc[idx, "question"]
    ref = df.loc[idx, "answer_sentences"]
    ans_llama = df.loc[idx, "llama3.1_answer"]
    ans_gemma = df.loc[idx, "gemma2_answer"]
    ans_mistral = df.loc[idx, "mistral_answer"]
    prompt = f'''question: {qn}\nreference: {ref}\nllama_answer: {ans_llama}\ngemma_answer: {ans_gemma}\nmistral_answer: {ans_mistral}'''
    return prompt

def gpt_review(client, idx):
    system_prompt =''' 
    You are an expert medical evaluator. Your task is to act as an impartial judge and evaluate the creative quality of a response generated by an AI model. We will provide you with a medical question, three different AI-generated answers (llama/gemma/mistral), and a set of evaluation criteria for creativity. You should first read the medical question carefully, then evaluate the quality of each of the responses with regard to each criterion. Answer the question asked in each criterion. You do not have to explain your verdict. Do not allow the length of the responses to influence your evaluation. Be as objective as possible.
    
    [The Start of Evaluation Criteria]
    
    [The Start of Conceptual Integration Criterion]
    Do the answers integrate multiple domains of medical knowledge (for example, clinical features, underlying pathophysiology, epidemiology) into a coherent narrative? If the answers show good conceptual integration, give a verdict of 1. Otherwise, give a verdict of 0.
    [The End of Conceptual Integration Criterion]
    
    [The Start of Associative Distance Criterion]
    Do the answers draw connections between diverse or non-obvious concepts by linking ideas from distinct knowledge areas? If the responses demonstrate a wide associative distance, give a verdict of 1. Otherwise, give a verdict of 0.
    [The End of Associative Distance Criterion]
    
    [The Start of Contextual Variability Criterion]
    Do the answers adapt the information to multiple contexts or scenarios (for example, considering variations in patient demographics or clinical settings) even when the question is general? If the responses display appropriate contextual variability, give a verdict of 1. Otherwise, give a verdict of 0.
    [The End of Contextual Variability Criterion]
    
    [The Start of Recombination of Ideas Criterion]
    Do the answers recombine known facts and ideas in novel ways to provide fresh perspectives on the question? If the responses effectively recombine ideas, give a verdict of 1. Otherwise, give a verdict of 0.
    [The End of Recombination of Ideas Criterion]
    
    [The Start of Perspective Shifting Criterion]
    Do the answers shift between different viewpoints or frames of reference (for example, clinical, pathophysiological, epidemiological)? If the responses exhibit effective perspective shifting, give a verdict of 1. Otherwise, give a verdict of 0.
    [The End of Perspective Shifting Criterion]
    
    [The End of Evaluation Criteria]

    [Output Format]
    Please provide your evaluation results in the following format by filling in the placeholders in []:
    "
    integration: [llama_verdict, gemma_verdict, mistral_verdict]
    association: [llama_verdict, gemma_verdict, mistral_verdict]
    context: [llama_verdict, gemma_verdict, mistral_verdict]
    recombination: [llama_verdict, gemma_verdict, mistral_verdict]
    perspective: [llama_verdict, gemma_verdict, mistral_verdict]
    "
    '''
    chat_completion = client.chat.completions.create(
        messages=[
            {   "role": "system",
                "content": system_prompt
            },
            {
                "role": "user",
                "content": write_prompt(idx),
            }
        ],
        model="gpt-4o-mini", # TODO: Change this to 4o
    )
    gpt_ans = chat_completion.choices[0].message.content
    # print(gpt_ans)
    return gpt_ans

def clean_verdict(text):
    """Extract criteria verdicts from GPT output."""
    pattern = r'(\w+): \[([\d\.]+),\s*([\d\.]+),\s*([\d\.]+)\]'
    matches = re.findall(pattern, text)
    
    results = {}
    for criterion, llama, gemma, mistral in matches:
        results[f'{criterion}_llama'] = float(llama)
        results[f'{criterion}_gemma'] = float(gemma)
        results[f'{criterion}_mistral'] = float(mistral)
    
    return pd.Series(results)

def save_checkpoint(df):
    """Save intermediate results to checkpoint."""
    df.to_csv(CHECKPOINT, index=False)

def load_checkpoint():
    """Load checkpoint if it exists."""
    if os.path.exists(CHECKPOINT):
        return pd.read_csv(CHECKPOINT)
    else:
        return None

def total_answer():
    llama_total = df['llama3.1_noa'].sum()
    gemma_total = df['gemma2_noa'].sum()
    mistral_total = df['mistral_noa'].sum()
    return {
        "llama": llama_total,
        "gemma": gemma_total,
        "mistral": mistral_total}

def calculate_percentage():
    percentages = {}
    total_answers = total_answer()
    for column in df.columns:
        if column.endswith(('_llama', '_gemma', '_mistral')):
            if column.startswith('correctness'):
                model = column.split('_')[1]
                total_sum = df[column].sum()
                percentage = (total_sum / total_answers[model]) * 100 if total_answers[model] > 0 else 0
            else:
                total_count = len(df[column])
                count_of_ones = (df[column] == 1).sum()
                percentage = (count_of_ones / total_count) * 100
            percentages[column] = percentage
    return percentages

# Main function
def main():
    global df
    client = initialize_judge()
    
    # Load checkpoint if it exists
    checkpoint_df = load_checkpoint()
    if checkpoint_df is not None:
        print("Checkpoint found. Resuming from last saved state.")
        df = checkpoint_df
    else:
        print("No checkpoint found. Starting fresh.")
        df = df_full

    # Evaluate rows
    completed_rows = df['gpt_judge_cre'].notna().sum() if 'gpt_judge_cre' in df else 0
    total_rows = len(df)
    for idx in tqdm(df.index[completed_rows:], desc="Evaluating with GPT"):
        df.at[idx, 'gpt_judge_cre'] = gpt_review(client, idx)
        evaluation_results = clean_verdict(df.at[idx, 'gpt_judge_cre'])
        
        for col, value in evaluation_results.items():
            df.at[idx, col] = value
        
        # Save checkpoint every 100 rows
        if (idx + 1) % 100 == 0 or idx == df.index[-1]:
            save_checkpoint(df)
            print(f"Checkpoint saved after {idx + 1} rows.")

    # Final save to output file
    df.to_csv(SAVEFILE, index=False)
    print(f"Final results saved to {SAVEFILE}.")

    # Calculate and print percentages
    percentages = calculate_percentage()
    for column, percentage in percentages.items():
        print(f"Percentage for {column}: {percentage:.2f}%")

# Run the script
if __name__ == "__main__":
    main()
